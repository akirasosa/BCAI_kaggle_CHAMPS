{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import pickle\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from time import time\n",
    "from typing import Dict, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_mean\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from my_lib.common.avg_meter import AverageMeterSet\n",
    "from my_lib.common.early_stopping import EarlyStopping\n",
    "from my_lib.torch.funcs import sqdist, batched_index_select\n",
    "from my_lib.torch.modules import MLP\n",
    "from my_lib.torch.optim import RAdam\n",
    "from proj import const\n",
    "from proj.loader import PandasDataset, atoms_collate_fn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.width = 999\n",
    "\n",
    "\n",
    "# %%\n",
    "@dataclasses.dataclass\n",
    "class Conf:\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "    clr_max_lr: float = 3e-3\n",
    "    clr_base_lr: float = 3e-6\n",
    "    clr_gamma: float = 0.999991\n",
    "\n",
    "    train_batch: int = 32\n",
    "    val_batch: int = 256\n",
    "\n",
    "    tformer_dim: int = 650\n",
    "    tformer_n_layers: int = 14\n",
    "    tformer_d_inner: int = 3800\n",
    "    tformer_dropout: float = 0.03\n",
    "    tformer_dropatt: float = 0.0\n",
    "    tformer_n_head: int = 10\n",
    "    tformer_wnorm: bool = True\n",
    "\n",
    "    optim: str = 'adam'\n",
    "    # loss: str = 'g_log_mae'\n",
    "\n",
    "    epochs: int = 400\n",
    "    is_save_epoch_fn: Callable = None\n",
    "    resume_from: Dict[str, int] = None\n",
    "\n",
    "    db_path: str = None\n",
    "\n",
    "    seed: int = 1\n",
    "\n",
    "    is_one_cv: bool = True\n",
    "\n",
    "    device: str = device\n",
    "\n",
    "    exp_name: str = 'simple_tformer'\n",
    "    exp_time: float = time()\n",
    "\n",
    "    logger_epoch = None\n",
    "    logger_step = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_logger(name, filename):\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        if not logger.hasHandlers():\n",
    "            logger.addHandler(logging.FileHandler(filename))\n",
    "        return logger\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.resume_from is not None:\n",
    "            assert self.out_dir.exists(), f'{self.out_dir} does not exist.'\n",
    "\n",
    "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.logger_epoch = self.create_logger(f'epoch_logger_{self.exp_time}', self.out_dir / 'epoch.log')\n",
    "        self.logger_step = self.create_logger(f'step_logger_{self.exp_time}', self.out_dir / 'step.log')\n",
    "\n",
    "        with (self.out_dir / 'conf.txt').open('w') as f:\n",
    "            f.write(str(self))\n",
    "\n",
    "        global device\n",
    "        device = self.device\n",
    "\n",
    "    @property\n",
    "    def out_dir(self) -> Path:\n",
    "        return const.DATA_DIR / 'experiments' / self.exp_name / str(self.exp_time)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(dataclasses.asdict(self))\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class AtomsData:\n",
    "    atom_type: torch.Tensor\n",
    "    atom_pos: torch.Tensor\n",
    "    scc_idx: torch.Tensor\n",
    "    scc_type: torch.Tensor\n",
    "    scc_val: torch.Tensor\n",
    "    scc_scaler: torch.Tensor\n",
    "\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_head, dropout=0.0, attn_dropout=0.0, wnorm=False, lev=0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.lev = lev\n",
    "\n",
    "        # To produce the query-key-value for the self-attention computation\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * d_model)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.proj1 = nn.Linear(d_model, d_inner)\n",
    "        self.proj2 = nn.Linear(d_inner, d_model)\n",
    "        self.gamma = nn.Parameter(torch.ones(1))  # For different sub-matrices of D\n",
    "        self.sqrtd = np.sqrt(d_head)\n",
    "\n",
    "        if wnorm:\n",
    "            self.wnorm()\n",
    "\n",
    "    def wnorm(self):\n",
    "        self.qkv_net = weight_norm(self.qkv_net, name=\"weight\")\n",
    "        self.o_net = weight_norm(self.o_net, name=\"weight\")\n",
    "        self.proj1 = weight_norm(self.proj1, name=\"weight\")\n",
    "        self.proj2 = weight_norm(self.proj2, name=\"weight\")\n",
    "\n",
    "    def forward(self, Z, D, new_mask, mask, store=False):\n",
    "        bsz, n_elem, nhid = Z.size()\n",
    "        n_head, d_head, d_model = self.n_head, self.d_head, self.d_model\n",
    "        assert nhid == d_model, \"Hidden dimension of Z does not agree with d_model\"\n",
    "\n",
    "        # Self-attention\n",
    "        inp = Z\n",
    "        Z = self.norm1(Z)\n",
    "        Z2, Z3, Z4 = self.qkv_net(Z).view(bsz, n_elem, n_head, 3 * d_head).chunk(3, dim=3)  # \"V, Q, K\"\n",
    "        W = torch.einsum('bnij, bmij->binm', Z3, Z4).type(D.dtype) / self.sqrtd\n",
    "        W = W + new_mask[:, None] - (self.gamma * D)[:, None]\n",
    "        W = self.attn_dropout(F.softmax(W, dim=3).type(mask.dtype) * mask[:, None])  # softmax(-gamma*D + Q^TK)\n",
    "        if store:\n",
    "            pickle.dump(W.cpu().detach().numpy(), open(f'analysis/layer_{self.lev}_W.pkl', 'wb'))\n",
    "        attn_out = torch.einsum('binm,bmij->bnij', W, Z2.type(W.dtype)).contiguous().view(bsz, n_elem, d_model)\n",
    "        attn_out = self.dropout(self.o_net(F.leaky_relu(attn_out)))\n",
    "        Z = attn_out + inp\n",
    "\n",
    "        # Position-wise feed-forward\n",
    "        inp = Z\n",
    "        Z = self.norm2(Z)\n",
    "\n",
    "        return self.proj2(self.dropout(F.relu(self.proj1(Z)))) + inp\n",
    "\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, dim, n_layers, d_inner,\n",
    "                 dropout=0.0,\n",
    "                 dropatt=0.0,\n",
    "                 n_head=10,\n",
    "                 wnorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.atom_embedding = nn.Embedding(const.N_ATOMS + 1, dim, padding_idx=0)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphLayer(\n",
    "                d_model=dim,\n",
    "                d_inner=d_inner,\n",
    "                n_head=n_head,\n",
    "                d_head=dim // n_head,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=dropatt,\n",
    "                wnorm=wnorm,\n",
    "                lev=i + 1,\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        self.pair_mlp = MLP(n_in=dim * 2, n_out=1, n_layers=2)\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def forward(self, inputs: AtomsData):\n",
    "        D = sqdist(inputs.atom_pos[:, :, :3],\n",
    "                   inputs.atom_pos[:, :, :3]).to(device)\n",
    "\n",
    "        mask = inputs.atom_type[:, :, 0] > 0\n",
    "        mask = torch.einsum('bi, bj->bij', mask, mask).type(inputs.atom_pos.dtype)\n",
    "\n",
    "        new_mask = -1e20 * torch.ones_like(mask).to(mask.device)\n",
    "        new_mask[mask > 0] = 0\n",
    "\n",
    "        Z = self.atom_embedding(inputs.atom_type[:, :, 0])\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            Z = self.layers[i](Z, D, new_mask, mask, store=False)\n",
    "\n",
    "        x_idx_0 = batched_index_select(Z, 1, inputs.scc_idx[:, :, 0])\n",
    "        x_idx_1 = batched_index_select(Z, 1, inputs.scc_idx[:, :, 1])\n",
    "        x_pair = torch.cat((x_idx_0, x_idx_1), dim=2)\n",
    "        y_pred = self.pair_mlp(x_pair)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(weight):\n",
    "        nn.init.uniform_(weight, -0.1, 0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_bias(bias):\n",
    "        nn.init.constant_(bias, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1 or classname.find('Conv1d') != -1:\n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                GraphTransformer.init_weight(m.weight)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                GraphTransformer.init_bias(m.bias)\n",
    "\n",
    "\n",
    "def calc_grouped_mae(y_pred, y_true, y_types, y_scaler):\n",
    "    y_pred_scaled = y_pred.squeeze(dim=2) * y_scaler[:, :, 1] + y_scaler[:, :, 0]\n",
    "    abs_err = (y_pred_scaled - y_true.squeeze(dim=2)).abs()\n",
    "    mae_types = scatter_mean(abs_err.view(-1), y_types.view(-1))[1:]  # 0 is pad\n",
    "    cnt_types = scatter_add(torch.ones_like(abs_err.view(-1)), y_types.view(-1))[1:]\n",
    "\n",
    "    return mae_types, cnt_types\n",
    "\n",
    "\n",
    "def run_on_step(batch, meters, model):\n",
    "    inputs = AtomsData(**{\n",
    "        k: v.to(device)\n",
    "        for k, v in batch.items()\n",
    "    })\n",
    "    y_pred = model(inputs)\n",
    "\n",
    "    mae_types, cnt_types = calc_grouped_mae(y_pred, inputs.scc_val, inputs.scc_type, inputs.scc_scaler)\n",
    "\n",
    "    # loss\n",
    "    # nonzero_indices = cnt_types.nonzero()\n",
    "    # loss = torch.log(mae_types[nonzero_indices] + 1e-9).mean()\n",
    "    n_pairs = cnt_types.sum()\n",
    "    loss = (mae_types * cnt_types).sum() / n_pairs\n",
    "    meters.update('loss', loss.item(), n_pairs.item())\n",
    "\n",
    "    for n, (mae, cnt) in enumerate(zip(mae_types, cnt_types)):\n",
    "        meters.update(f'mae_{const.TYPES[n]}', mae.item(), cnt.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_after_step(meters):\n",
    "    # log mae for each types\n",
    "    lmae_types = {\n",
    "        f'lmae_{t}': np.log(meters[f'mae_{t}'].avg)\n",
    "        for t in const.TYPES\n",
    "    }\n",
    "\n",
    "    # competition metric\n",
    "    mean_lmae = np.log([meters[f'mae_{t}'].avg for t in const.TYPES]).mean()\n",
    "\n",
    "    return {\n",
    "        **lmae_types,\n",
    "        'mean_lmae': mean_lmae,\n",
    "    }\n",
    "\n",
    "\n",
    "def train(loader, model: nn.Module, optimizer: Optimizer, scheduler, conf: Conf, prefix: str = 'train'):\n",
    "    meters = AverageMeterSet()\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, leave=False)):\n",
    "        meters.update('lr', optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        loss = run_on_step(batch, meters, model)\n",
    "        conf.logger_step.debug(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    metrics = run_after_step(meters)\n",
    "\n",
    "    return {\n",
    "        'lr': meters['lr'].avg,\n",
    "        f'{prefix}_loss': meters['loss'].avg,\n",
    "        **{\n",
    "            f'{prefix}_{k}': v\n",
    "            for k, v in metrics.items()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(loader, model: nn.Module, conf: Conf, prefix: str = 'val'):\n",
    "    meters = AverageMeterSet()\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, leave=False)):\n",
    "        with torch.no_grad():\n",
    "            run_on_step(batch, meters, model)\n",
    "\n",
    "    metrics = run_after_step(meters)\n",
    "\n",
    "    return {\n",
    "        f'{prefix}_loss': meters['loss'].avg,\n",
    "        **{\n",
    "            f'{prefix}_{k}': v\n",
    "            for k, v in metrics.items()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def log_hist(df_hist: pd.DataFrame, logger: logging.Logger):\n",
    "    last = df_hist.tail(1)\n",
    "    best = df_hist.sort_values('val_mean_lmae', ascending=True).head(1)\n",
    "    summary = pd.concat((last, best)).reset_index(drop=True)\n",
    "    summary['name'] = ['Last', 'Best']\n",
    "    logger.debug(summary[[\n",
    "                             'name',\n",
    "                             'epoch',\n",
    "                             'train_loss',\n",
    "                             'val_loss',\n",
    "                             'train_mean_lmae',\n",
    "                             'val_mean_lmae',\n",
    "                         ] + [\n",
    "                             f'train_lmae_{t}' for t in const.TYPES\n",
    "                         ] + [\n",
    "                             f'val_lmae_{t}' for t in const.TYPES\n",
    "                         ]])\n",
    "    logger.debug('')\n",
    "\n",
    "\n",
    "def write_on_board(df_hist: pd.DataFrame, writer: SummaryWriter, conf: Conf):\n",
    "    row = df_hist.tail(1).iloc[0]\n",
    "\n",
    "    writer.add_scalars(f'{conf.exp_name}/lr', {\n",
    "        f'{conf.exp_time}': row.lr,\n",
    "    }, row.epoch)\n",
    "\n",
    "    writer.add_scalars(f'{conf.exp_name}/loss/coupling/total', {\n",
    "        f'{conf.exp_time}_train': row.train_loss,\n",
    "        f'{conf.exp_time}_val': row.val_loss,\n",
    "    }, row.epoch)\n",
    "\n",
    "    for tag in const.TYPES:\n",
    "        writer.add_scalars(f'{conf.exp_name}/metric/type/{tag}', {\n",
    "            f'{conf.exp_time}_train': row[f'train_lmae_{tag}'],\n",
    "            f'{conf.exp_time}_val': row[f'val_lmae_{tag}'],\n",
    "        }, row.epoch)\n",
    "    writer.add_scalars(f'{conf.exp_name}/metric/type/total', {\n",
    "        f'{conf.exp_time}_train': row['train_mean_lmae'],\n",
    "        f'{conf.exp_time}_val': row['val_mean_lmae'],\n",
    "    }, row.epoch)\n",
    "\n",
    "\n",
    "def main(conf: Conf):\n",
    "    print(conf)\n",
    "    print(f'less +F {conf.out_dir}/epoch.log')\n",
    "\n",
    "    df = pd.read_pickle(conf.db_path)\n",
    "    df = df[~df.is_test]\n",
    "\n",
    "    folds = KFold(n_splits=4, random_state=conf.seed, shuffle=True)\n",
    "\n",
    "    for cv, (train_idx, val_idx) in enumerate(folds.split(df)):\n",
    "        df_train = df.iloc[train_idx]\n",
    "        df_val = df.iloc[val_idx]\n",
    "        print(cv, len(df_train), len(df_val))\n",
    "\n",
    "        train_loader = DataLoader(PandasDataset(df_train),\n",
    "                                  batch_size=conf.train_batch,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=cpu_count() - 1,\n",
    "                                  collate_fn=atoms_collate_fn)\n",
    "        val_loader = DataLoader(PandasDataset(df_val),\n",
    "                                batch_size=conf.val_batch,\n",
    "                                shuffle=False,\n",
    "                                num_workers=cpu_count() - 1,\n",
    "                                collate_fn=atoms_collate_fn)\n",
    "\n",
    "        model = GraphTransformer(\n",
    "            dim=conf.tformer_dim,\n",
    "            n_layers=conf.tformer_n_layers,\n",
    "            d_inner=conf.tformer_d_inner,\n",
    "            dropout=conf.tformer_dropatt,\n",
    "            dropatt=conf.tformer_dropatt,\n",
    "            n_head=conf.tformer_n_head,\n",
    "            wnorm=conf.tformer_wnorm,\n",
    "        ).to(device)\n",
    "\n",
    "        if conf.optim == 'adam':\n",
    "            opt = Adam(model.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "        elif conf.optim == 'radam':\n",
    "            opt = RAdam(model.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "        else:\n",
    "            raise Exception(f'Not supported optim {conf.optim}')\n",
    "        scheduler = CyclicLR(\n",
    "            opt,\n",
    "            base_lr=conf.clr_base_lr,\n",
    "            max_lr=conf.clr_max_lr,\n",
    "            step_size_up=len(train_loader) * 10,\n",
    "            mode=\"exp_range\",\n",
    "            gamma=conf.clr_gamma,\n",
    "            cycle_momentum=False,\n",
    "        )\n",
    "        early_stopping = EarlyStopping(patience=100)\n",
    "\n",
    "        if conf.resume_from is not None:\n",
    "            cv_resume = conf.resume_from['cv']\n",
    "            start_epoch = conf.resume_from['epoch']\n",
    "            if cv < cv_resume:\n",
    "                continue\n",
    "            ckpt = torch.load(f'{conf.out_dir}/{cv}-{start_epoch:03d}.ckpt')\n",
    "            model.load_state_dict(ckpt['model'])\n",
    "            opt.load_state_dict(ckpt['optimizer'])\n",
    "            scheduler.load_state_dict(ckpt['scheduler'])\n",
    "            writer = SummaryWriter(logdir=ckpt['writer_logdir'], purge_step=start_epoch)\n",
    "            hist = pd.read_csv(f'{conf.out_dir}/{cv}.csv').to_dict('records')\n",
    "            print(f'Loaded checkpoint cv {cv}, epoch {start_epoch} from {conf.out_dir}')\n",
    "        else:\n",
    "            hist = []\n",
    "            writer = SummaryWriter(logdir=str(conf.out_dir / 'tb_log'))\n",
    "            start_epoch = 0\n",
    "\n",
    "        for epoch in range(start_epoch, conf.epochs):\n",
    "            train_result = train(train_loader, model, opt, scheduler, conf)\n",
    "            val_result = validate(val_loader, model, conf)\n",
    "            result = {\n",
    "                'epoch': epoch,\n",
    "                **train_result,\n",
    "                **val_result,\n",
    "            }\n",
    "            hist.append(result)\n",
    "            df_hist = pd.DataFrame(hist)\n",
    "\n",
    "            log_hist(df_hist, conf.logger_epoch)\n",
    "            write_on_board(df_hist, writer, conf)\n",
    "\n",
    "            if epoch % 10 == 9:\n",
    "                for name, param in model.named_parameters():\n",
    "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "\n",
    "            if conf.is_save_epoch_fn is not None and conf.is_save_epoch_fn(epoch):\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': opt.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'writer_logdir': writer.logdir,\n",
    "                }, f'{conf.out_dir}/{cv}-{epoch + 1:03d}.ckpt')\n",
    "                df_hist.to_csv(f'{conf.out_dir}/{cv}.csv')\n",
    "                print(f'Saved checkpoint {conf.out_dir}/{cv}-{epoch + 1:03d}.ckpt')\n",
    "\n",
    "            should_stop = early_stopping.step(result['val_mean_lmae'])\n",
    "            if should_stop:\n",
    "                print(f'Early stopping at {epoch}')\n",
    "                break\n",
    "\n",
    "        df_hist = pd.DataFrame(hist)\n",
    "        best = df_hist.sort_values('val_mean_lmae', ascending=True).head(1).iloc[0]\n",
    "        print(best)\n",
    "\n",
    "        writer.close()\n",
    "        if conf.is_one_cv:\n",
    "            break\n",
    "\n",
    "\n",
    "# %%\n",
    "conf = Conf(\n",
    "    is_one_cv=True,\n",
    "\n",
    "    device='cuda',\n",
    "\n",
    "    train_batch=32,\n",
    "    val_batch=256,\n",
    "\n",
    "    lr=1e-4,\n",
    "    clr_max_lr=3e-3,\n",
    "    clr_base_lr=3e-6,\n",
    "    # lr=3e-5,\n",
    "    # clr_max_lr=1e-3,\n",
    "    # clr_base_lr=1e-6,\n",
    "    clr_gamma=0.999991,\n",
    "    weight_decay=1e-4,\n",
    "\n",
    "    tformer_dim=300,\n",
    "    tformer_n_layers=7,\n",
    "    tformer_d_inner=3800,\n",
    "    tformer_dropout=0.03,\n",
    "    tformer_dropatt=0.0,\n",
    "    tformer_n_head=10,\n",
    "    tformer_wnorm=True,\n",
    "\n",
    "    epochs=400,\n",
    "\n",
    "    db_path=const.DATA_DIR / 'artifacts' / 'data.pkl',\n",
    "\n",
    "    exp_time=time(),\n",
    ")\n",
    "# main(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Untitled.ipynb to python\n",
      "[NbConvertApp] Writing 17109 bytes to /home/akirasosa/data/champs-scalar-coupling/experiments/simple_tformer/1568976855.513035/Untitled.py\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$conf.out_dir\"\n",
    "jupyter nbconvert --output-dir=\"$1\" --to=python Untitled.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clr_base_lr': 3e-06,\n",
      " 'clr_gamma': 0.999991,\n",
      " 'clr_max_lr': 0.003,\n",
      " 'db_path': PosixPath('/home/akirasosa/data/champs-scalar-coupling/artifacts/data.pkl'),\n",
      " 'device': 'cuda',\n",
      " 'epochs': 400,\n",
      " 'exp_name': 'simple_tformer',\n",
      " 'exp_time': 1568976855.513035,\n",
      " 'is_one_cv': True,\n",
      " 'is_save_epoch_fn': None,\n",
      " 'lr': 0.0001,\n",
      " 'optim': 'adam',\n",
      " 'resume_from': None,\n",
      " 'seed': 1,\n",
      " 'tformer_d_inner': 3800,\n",
      " 'tformer_dim': 300,\n",
      " 'tformer_dropatt': 0.0,\n",
      " 'tformer_dropout': 0.03,\n",
      " 'tformer_n_head': 10,\n",
      " 'tformer_n_layers': 7,\n",
      " 'tformer_wnorm': True,\n",
      " 'train_batch': 32,\n",
      " 'val_batch': 256,\n",
      " 'weight_decay': 0.0001}\n",
      "less +F /home/akirasosa/data/champs-scalar-coupling/experiments/simple_tformer/1568976855.513035/epoch.log\n",
      "0 63752 21251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1993), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1993), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Warning: NaN or Inf found in input tensor.\n",
      "Early stopping at 1\n",
      "epoch              0.000000\n",
      "lr                 0.000151\n",
      "train_loss         3.069604\n",
      "train_lmae_1JHC    1.928725\n",
      "train_lmae_1JHN    1.799505\n",
      "train_lmae_2JHC    0.782115\n",
      "train_lmae_2JHH    0.739016\n",
      "train_lmae_2JHN    0.698148\n",
      "train_lmae_3JHC    0.919817\n",
      "train_lmae_3JHH    1.038199\n",
      "train_lmae_3JHN    0.028080\n",
      "train_mean_lmae    0.991701\n",
      "val_loss           2.522684\n",
      "val_lmae_1JHC      1.605826\n",
      "val_lmae_1JHN      1.227855\n",
      "val_lmae_2JHC      0.542248\n",
      "val_lmae_2JHH      0.453682\n",
      "val_lmae_2JHN      0.322650\n",
      "val_lmae_3JHC      0.874606\n",
      "val_lmae_3JHH      0.981594\n",
      "val_lmae_3JHN     -0.087247\n",
      "val_mean_lmae      0.740152\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "main(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
